import nltk

def tokenize(sentence):
    pass
def stem(word):
    pass
def bad_of_words(tokenize_sentence, all_words):
    pass